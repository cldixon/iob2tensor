{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"iob2labels Convert IOB2-format NER span annotations into integer label sequences for Transformer-based token classification tasks \u2014 and convert them back. If you use annotation tools like Prodigy , Label Studio , or Doccano to annotate NER data, this library converts those character-offset span annotations into the label arrays you need for training. At inference time, it converts model predictions back into span annotations. Installation uv add iob2labels Dependencies: tokenizers (HuggingFace Rust-backed tokenizer) and pydantic . No torch or transformers required. Quick Start from iob2labels import IOB2Encoder encoder = IOB2Encoder ( labels = [ \"actor\" , \"character\" , \"plot\" ], tokenizer = \"bert-base-uncased\" , ) ## -- encoding: spans \u2192 labels (for training) labels = encoder ( text = \"Did Dame Judy Dench star in a British film about Queen Elizabeth?\" , spans = [ { \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }, { \"label\" : \"plot\" , \"start\" : 30 , \"end\" : 37 }, { \"label\" : \"character\" , \"start\" : 49 , \"end\" : 64 }, ] ) # >>> [-100, 0, 1, 2, 2, 2, 0, 0, 0, 5, 0, 0, 3, 4, 0, -100] ## -- decoding: labels \u2192 spans (for inference) spans = encoder . decode_text ( labels , \"Did Dame Judy Dench star in a British film about Queen Elizabeth?\" ) # >>> [{\"start\": 4, \"end\": 19, \"label\": \"actor\"}, {\"start\": 30, \"end\": 37, \"label\": \"plot\"}, {\"start\": 49, \"end\": 64, \"label\": \"character\"}] Example pulled from the MITMovie dataset. The output is a list[int] aligned to the tokenizer's output. Convert to a tensor or array as needed: import torch x = torch . tensor ( labels ) # or with numpy import numpy as np x = np . array ( labels ) How It Works The IOB2 format assigns each token one of three tag types: O (Outside) - not part of any entity B-LABEL (Beginning) - first token of an entity I-LABEL (Inside) - continuation of an entity Each entity class generates 2 labels (B + I), plus the O class, so the total label count is always (n * 2) + 1 : encoder . label_map # >>> {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4, 'B-PLOT': 5, 'I-PLOT': 6} Special tokens (e.g., [CLS] , [SEP] ) receive the ignore value -100 , which PyTorch's CrossEntropyLoss skips by default. Decoding At inference time, convert model predictions back into character-offset span annotations: ## -- decode_text: pass the raw text (tokenizes internally) spans = encoder . decode_text ( predicted_labels , text ) ## -- decode: pass a pre-built Encoding object (avoids re-tokenizing) encoding = encoder . tokenizer . encode ( text ) spans = encoder . decode ( predicted_labels , encoding , text ) Both return list[Span] \u2014 a list of typed dicts with start , end , and label fields. The decoder handles SentencePiece-style tokenizers (ALBERT, XLNet, T5, XLM-RoBERTa, CamemBERT) that absorb leading whitespace into tokens (e.g., \u2581Queen maps to chars (48, 54) instead of (49, 54) ). Character offsets are corrected automatically using the original text. Tokenizer Input The tokenizer argument accepts three forms: # 1. checkpoint name (downloads from HuggingFace Hub) encoder = IOB2Encoder ( labels = labels , tokenizer = \"bert-base-uncased\" ) # 2. standalone tokenizers.Tokenizer instance from tokenizers import Tokenizer tok = Tokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) # 3. transformers PreTrainedTokenizerFast (unwrapped automatically) from transformers import AutoTokenizer tok = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) When using a checkpoint string not in the tested list , a UserWarning is emitted. The tokenizer will still load and may work correctly, but round-trip correctness has not been verified. Batch Encoding annotations = [ { \"text\" : \"Did Dame Judy Dench star?\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }]}, { \"text\" : \"Matt Damon was Jason Bourne.\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 0 , \"end\" : 10 }]}, ] results = encoder . batch ( annotations ) # >>> [[-100, 0, 1, 2, 2, 2, 0, -100], [-100, 1, 2, 0, 0, 0, 0, -100]] The batch path uses the Rust-backed encode_batch() for parallelized tokenization. Returns list[list[int]] with no padding; use HuggingFace's DataCollatorForTokenClassification or your own padding for training. Custom Field Names If your annotation data uses non-standard field names, configure them at construction: # BioMed-NER dataset uses \"entities\" and \"class\" instead of \"spans\" and \"label\" encoder = IOB2Encoder ( labels = [ \"organism\" , \"chemicals\" ], tokenizer = \"bert-base-uncased\" , spans_field = \"entities\" , label_field = \"class\" , ) Annotation Validation Input annotations are validated upfront with clear error messages for common mistakes: Negative character offsets Inverted spans ( start >= end ) Spans extending past the text length Overlapping spans (IOB2 does not support overlapping entities) encoder ( text = \"Hello\" , spans = [{ \"label\" : \"test\" , \"start\" : 0 , \"end\" : 100 }]) # ValueError: Span 0 ('test') extends past the text (end=100, text length=5). # Ensure character offsets are within the text bounds. Built-in Conversion Check By default, every encoding is verified by recovering the entity text from the produced labels and comparing it to the original annotation. This catches misalignment bugs early. Disable it for performance in production: encoder = IOB2Encoder ( labels = labels , tokenizer = tok , conversion_check = False ) Supported Tokenizers Tested across four tokenizer families: Family Checkpoints WordPiece bert-base-cased , bert-base-uncased , bert-large-cased , bert-large-uncased , bert-base-multilingual-cased , distilbert-base-cased , distilbert-base-uncased , google/electra-base-discriminator BPE (byte-level) roberta-base , roberta-large , distilroberta-base , allenai/longformer-base-4096 SentencePiece BPE FacebookAI/xlm-roberta-base , almanach/camembert-base SentencePiece Unigram albert-base-v2 , xlnet-base-cased , t5-small , google/flan-t5-base Other HuggingFace-compatible tokenizers with a tokenizer.json file on the Hub should work as well. A UserWarning is emitted for untested checkpoints, and the built-in conversion check will flag any alignment issues. Tests uv run pytest tests/ -v 313 tests across 10 test files, including: Encoding matrix : every supported tokenizer \u00d7 standard, multi-entity, and edge case annotations (entity at start/end of text, adjacent entities, punctuation) Decoding round-trips : encode \u2192 decode \u2192 assert recovered spans exactly match originals, across all 18 tokenizers. This is the strongest correctness guarantee \u2014 if the round-trip holds, both the encoder and decoder are correct for that tokenizer. Annotation validation : negative offsets, inverted spans, overlapping entities, spans past text bounds Tokenizer warning : untested checkpoint detection SentencePiece whitespace handling : tokenizers like google/flan-t5-base absorb leading spaces into tokens (e.g., \u2581Queen \u2192 chars (48, 54) instead of (49, 54) ). The decoder corrects these offsets, verified by round-trip tests across all SentencePiece checkpoints.","title":"Home"},{"location":"#iob2labels","text":"Convert IOB2-format NER span annotations into integer label sequences for Transformer-based token classification tasks \u2014 and convert them back. If you use annotation tools like Prodigy , Label Studio , or Doccano to annotate NER data, this library converts those character-offset span annotations into the label arrays you need for training. At inference time, it converts model predictions back into span annotations.","title":"iob2labels"},{"location":"#installation","text":"uv add iob2labels Dependencies: tokenizers (HuggingFace Rust-backed tokenizer) and pydantic . No torch or transformers required.","title":"Installation"},{"location":"#quick-start","text":"from iob2labels import IOB2Encoder encoder = IOB2Encoder ( labels = [ \"actor\" , \"character\" , \"plot\" ], tokenizer = \"bert-base-uncased\" , ) ## -- encoding: spans \u2192 labels (for training) labels = encoder ( text = \"Did Dame Judy Dench star in a British film about Queen Elizabeth?\" , spans = [ { \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }, { \"label\" : \"plot\" , \"start\" : 30 , \"end\" : 37 }, { \"label\" : \"character\" , \"start\" : 49 , \"end\" : 64 }, ] ) # >>> [-100, 0, 1, 2, 2, 2, 0, 0, 0, 5, 0, 0, 3, 4, 0, -100] ## -- decoding: labels \u2192 spans (for inference) spans = encoder . decode_text ( labels , \"Did Dame Judy Dench star in a British film about Queen Elizabeth?\" ) # >>> [{\"start\": 4, \"end\": 19, \"label\": \"actor\"}, {\"start\": 30, \"end\": 37, \"label\": \"plot\"}, {\"start\": 49, \"end\": 64, \"label\": \"character\"}] Example pulled from the MITMovie dataset. The output is a list[int] aligned to the tokenizer's output. Convert to a tensor or array as needed: import torch x = torch . tensor ( labels ) # or with numpy import numpy as np x = np . array ( labels )","title":"Quick Start"},{"location":"#how-it-works","text":"The IOB2 format assigns each token one of three tag types: O (Outside) - not part of any entity B-LABEL (Beginning) - first token of an entity I-LABEL (Inside) - continuation of an entity Each entity class generates 2 labels (B + I), plus the O class, so the total label count is always (n * 2) + 1 : encoder . label_map # >>> {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4, 'B-PLOT': 5, 'I-PLOT': 6} Special tokens (e.g., [CLS] , [SEP] ) receive the ignore value -100 , which PyTorch's CrossEntropyLoss skips by default.","title":"How It Works"},{"location":"#decoding","text":"At inference time, convert model predictions back into character-offset span annotations: ## -- decode_text: pass the raw text (tokenizes internally) spans = encoder . decode_text ( predicted_labels , text ) ## -- decode: pass a pre-built Encoding object (avoids re-tokenizing) encoding = encoder . tokenizer . encode ( text ) spans = encoder . decode ( predicted_labels , encoding , text ) Both return list[Span] \u2014 a list of typed dicts with start , end , and label fields. The decoder handles SentencePiece-style tokenizers (ALBERT, XLNet, T5, XLM-RoBERTa, CamemBERT) that absorb leading whitespace into tokens (e.g., \u2581Queen maps to chars (48, 54) instead of (49, 54) ). Character offsets are corrected automatically using the original text.","title":"Decoding"},{"location":"#tokenizer-input","text":"The tokenizer argument accepts three forms: # 1. checkpoint name (downloads from HuggingFace Hub) encoder = IOB2Encoder ( labels = labels , tokenizer = \"bert-base-uncased\" ) # 2. standalone tokenizers.Tokenizer instance from tokenizers import Tokenizer tok = Tokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) # 3. transformers PreTrainedTokenizerFast (unwrapped automatically) from transformers import AutoTokenizer tok = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) When using a checkpoint string not in the tested list , a UserWarning is emitted. The tokenizer will still load and may work correctly, but round-trip correctness has not been verified.","title":"Tokenizer Input"},{"location":"#batch-encoding","text":"annotations = [ { \"text\" : \"Did Dame Judy Dench star?\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }]}, { \"text\" : \"Matt Damon was Jason Bourne.\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 0 , \"end\" : 10 }]}, ] results = encoder . batch ( annotations ) # >>> [[-100, 0, 1, 2, 2, 2, 0, -100], [-100, 1, 2, 0, 0, 0, 0, -100]] The batch path uses the Rust-backed encode_batch() for parallelized tokenization. Returns list[list[int]] with no padding; use HuggingFace's DataCollatorForTokenClassification or your own padding for training.","title":"Batch Encoding"},{"location":"#custom-field-names","text":"If your annotation data uses non-standard field names, configure them at construction: # BioMed-NER dataset uses \"entities\" and \"class\" instead of \"spans\" and \"label\" encoder = IOB2Encoder ( labels = [ \"organism\" , \"chemicals\" ], tokenizer = \"bert-base-uncased\" , spans_field = \"entities\" , label_field = \"class\" , )","title":"Custom Field Names"},{"location":"#annotation-validation","text":"Input annotations are validated upfront with clear error messages for common mistakes: Negative character offsets Inverted spans ( start >= end ) Spans extending past the text length Overlapping spans (IOB2 does not support overlapping entities) encoder ( text = \"Hello\" , spans = [{ \"label\" : \"test\" , \"start\" : 0 , \"end\" : 100 }]) # ValueError: Span 0 ('test') extends past the text (end=100, text length=5). # Ensure character offsets are within the text bounds.","title":"Annotation Validation"},{"location":"#built-in-conversion-check","text":"By default, every encoding is verified by recovering the entity text from the produced labels and comparing it to the original annotation. This catches misalignment bugs early. Disable it for performance in production: encoder = IOB2Encoder ( labels = labels , tokenizer = tok , conversion_check = False )","title":"Built-in Conversion Check"},{"location":"#supported-tokenizers","text":"Tested across four tokenizer families: Family Checkpoints WordPiece bert-base-cased , bert-base-uncased , bert-large-cased , bert-large-uncased , bert-base-multilingual-cased , distilbert-base-cased , distilbert-base-uncased , google/electra-base-discriminator BPE (byte-level) roberta-base , roberta-large , distilroberta-base , allenai/longformer-base-4096 SentencePiece BPE FacebookAI/xlm-roberta-base , almanach/camembert-base SentencePiece Unigram albert-base-v2 , xlnet-base-cased , t5-small , google/flan-t5-base Other HuggingFace-compatible tokenizers with a tokenizer.json file on the Hub should work as well. A UserWarning is emitted for untested checkpoints, and the built-in conversion check will flag any alignment issues.","title":"Supported Tokenizers"},{"location":"#tests","text":"uv run pytest tests/ -v 313 tests across 10 test files, including: Encoding matrix : every supported tokenizer \u00d7 standard, multi-entity, and edge case annotations (entity at start/end of text, adjacent entities, punctuation) Decoding round-trips : encode \u2192 decode \u2192 assert recovered spans exactly match originals, across all 18 tokenizers. This is the strongest correctness guarantee \u2014 if the round-trip holds, both the encoder and decoder are correct for that tokenizer. Annotation validation : negative offsets, inverted spans, overlapping entities, spans past text bounds Tokenizer warning : untested checkpoint detection SentencePiece whitespace handling : tokenizers like google/flan-t5-base absorb leading spaces into tokens (e.g., \u2581Queen \u2192 chars (48, 54) instead of (49, 54) ). The decoder corrects these offsets, verified by round-trip tests across all SentencePiece checkpoints.","title":"Tests"},{"location":"api/","text":"API Reference IOB2Encoder The main interface for encoding and decoding IOB2 NER annotations. from iob2labels import IOB2Encoder Constructor IOB2Encoder ( labels : list [ str ], tokenizer : str | Tokenizer , * , ignore_token : int = - 100 , ends_at_next_char : bool = True , conversion_check : bool = True , max_length : int | None = 512 , start_field : str = \"start\" , end_field : str = \"end\" , label_field : str = \"label\" , text_field : str = \"text\" , spans_field : str = \"spans\" , ) Parameters: Parameter Type Default Description labels list[str] required Entity class names (e.g., [\"actor\", \"character\", \"plot\"] ). Each generates B- and I- IOB2 tags. tokenizer str \\| Tokenizer required HuggingFace checkpoint name, tokenizers.Tokenizer , or transformers.PreTrainedTokenizerFast . See Tokenizer Input . ignore_token int -100 Label value assigned to special tokens. PyTorch's CrossEntropyLoss ignores this by default. ends_at_next_char bool True Whether span end offsets point to the character after the last entity character (standard convention). conversion_check bool True Run a round-trip verification after each encoding to catch misalignment bugs. max_length int \\| None 512 Maximum token sequence length. Enables truncation on the tokenizer. Set to None to disable. start_field str \"start\" Key for start offset in span dicts. end_field str \"end\" Key for end offset in span dicts. label_field str \"label\" Key for entity label in span dicts. text_field str \"text\" Key for text string in batch annotation dicts. spans_field str \"spans\" Key for spans list in batch annotation dicts. Properties label_map -> dict[str, int] The IOB2 label-to-index mapping. Returns a copy to prevent mutation. encoder . label_map # {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4, 'B-PLOT': 5, 'I-PLOT': 6} tokenizer -> Tokenizer The resolved tokenizers.Tokenizer instance. Methods __call__(text, spans) -> list[int] Encode a single annotation into IOB2 label indices. labels = encoder ( text = \"Did Dame Judy Dench star?\" , spans = [{ \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }], ) Parameters: Parameter Type Description text str The raw input text. spans list[dict] Span dicts with start/end/label fields (field names configurable via constructor). Returns: list[int] of IOB2 labels aligned to tokenizer output. batch(annotations, *, on_error=\"raise\") -> list[list[int]] Encode a batch of annotations. Uses Rust-backed encode_batch() for parallelized tokenization. results = encoder . batch ( annotations , on_error = \"skip\" ) Parameters: Parameter Type Default Description annotations list[dict] required Annotation dicts with text/spans fields. on_error str \"raise\" \"raise\" to fail on first error, \"skip\" to skip failed annotations. Returns: list[list[int]] of IOB2 label sequences (unpadded). decode(labels, encoding, text) -> list[Span] Recover span annotations from IOB2 label indices given a pre-built Encoding object. encoding = encoder . tokenizer . encode ( text ) spans = encoder . decode ( predicted_labels , encoding , text ) Parameters: Parameter Type Description labels list[int] IOB2 label indices (e.g., model predictions after argmax). encoding Encoding The tokenizers.Encoding object for the text. text str The original text (needed to resolve SentencePiece whitespace boundaries). Returns: list[Span] with character-level start , end , and label fields. decode_text(labels, text) -> list[Span] Convenience method: tokenizes the text internally, then decodes. spans = encoder . decode_text ( predicted_labels , text ) Parameters: Parameter Type Description labels list[int] IOB2 label indices. text str The raw input text (will be tokenized internally). Returns: list[Span] with character-level start , end , and label fields. Types Span A TypedDict representing a single entity annotation with character offsets. from iob2labels import Span Field Type Description start int Start character offset (inclusive). end int End character offset (exclusive). label str Entity class name. Annotation A TypedDict representing a text with its entity annotations. from iob2labels import Annotation Field Type Description text str The annotated text. spans list[Span] Entity annotations. Utility Functions create_label_map(labels) -> dict[str, int] Build an IOB2 label-to-index mapping from a list of entity class names. from iob2labels import create_label_map label_map = create_label_map ([ \"actor\" , \"character\" ]) # {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4} Parameters: Parameter Type Default Description labels list[str] \\| None None Entity class names. Defaults to [\"LABEL\"] if None . format_entity_label(prefix, label) -> str Format an IOB2 entity label string. from iob2labels import format_entity_label format_entity_label ( \"B\" , \"actor\" ) # 'B-ACTOR' Parameters: Parameter Type Description prefix \"B\" \\| \"I\" IOB2 prefix (Beginning or Inside). label str Entity class name. preprocessing(text, spans, ...) -> Annotation Validate and normalize annotation data via Pydantic, then return as a typed dict. from iob2labels import preprocessing annotation = preprocessing ( text = \"Hello world\" , spans = [{ \"start\" : 0 , \"end\" : 5 , \"label\" : \"greeting\" }], ) Parameters: Parameter Type Default Description text str required The input text. spans list[dict] required Span dicts to validate. start_field str \"start\" Key for start offset. end_field str \"end\" Key for end offset. label_field str \"label\" Key for entity label. Raises: ValidationError for type mismatches, ValueError for invalid span geometry. check_iob_conversion(...) -> None Verify that encoded IOB2 labels correctly recover the original entity text. Used internally when conversion_check=True . from iob2labels import check_iob_conversion check_iob_conversion ( iob_labels = labels , label_map = encoder . label_map , tokenizer = encoder . tokenizer , input_ids = encoding . ids , annotation = annotation , ) Parameters: Parameter Type Description iob_labels list[int] The encoded label sequence to verify. label_map dict[str, int] IOB2 label-to-index mapping. tokenizer Tokenizer The tokenizer instance. input_ids list[int] Token IDs from the encoding. annotation Annotation The original annotation for comparison. Raises: AssertionError if the recovered entities do not match the original spans. get_entity_index_ranges(label_map, iob_labels) -> list[tuple[int, int]] Extract token index ranges for each entity from an IOB2 label sequence. from iob2labels import get_entity_index_ranges ranges = get_entity_index_ranges ( encoder . label_map , labels ) # [(1, 4), (8, 8), (11, 12)] \u2014 (start_token_idx, end_token_idx) for each entity Parameters: Parameter Type Description label_map dict[str, int] IOB2 label-to-index mapping. iob_labels list[int] The IOB2 label sequence to scan.","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#iob2encoder","text":"The main interface for encoding and decoding IOB2 NER annotations. from iob2labels import IOB2Encoder","title":"IOB2Encoder"},{"location":"api/#constructor","text":"IOB2Encoder ( labels : list [ str ], tokenizer : str | Tokenizer , * , ignore_token : int = - 100 , ends_at_next_char : bool = True , conversion_check : bool = True , max_length : int | None = 512 , start_field : str = \"start\" , end_field : str = \"end\" , label_field : str = \"label\" , text_field : str = \"text\" , spans_field : str = \"spans\" , ) Parameters: Parameter Type Default Description labels list[str] required Entity class names (e.g., [\"actor\", \"character\", \"plot\"] ). Each generates B- and I- IOB2 tags. tokenizer str \\| Tokenizer required HuggingFace checkpoint name, tokenizers.Tokenizer , or transformers.PreTrainedTokenizerFast . See Tokenizer Input . ignore_token int -100 Label value assigned to special tokens. PyTorch's CrossEntropyLoss ignores this by default. ends_at_next_char bool True Whether span end offsets point to the character after the last entity character (standard convention). conversion_check bool True Run a round-trip verification after each encoding to catch misalignment bugs. max_length int \\| None 512 Maximum token sequence length. Enables truncation on the tokenizer. Set to None to disable. start_field str \"start\" Key for start offset in span dicts. end_field str \"end\" Key for end offset in span dicts. label_field str \"label\" Key for entity label in span dicts. text_field str \"text\" Key for text string in batch annotation dicts. spans_field str \"spans\" Key for spans list in batch annotation dicts.","title":"Constructor"},{"location":"api/#properties","text":"","title":"Properties"},{"location":"api/#label_map-dictstr-int","text":"The IOB2 label-to-index mapping. Returns a copy to prevent mutation. encoder . label_map # {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4, 'B-PLOT': 5, 'I-PLOT': 6}","title":"label_map -&gt; dict[str, int]"},{"location":"api/#tokenizer-tokenizer","text":"The resolved tokenizers.Tokenizer instance.","title":"tokenizer -&gt; Tokenizer"},{"location":"api/#methods","text":"","title":"Methods"},{"location":"api/#__call__text-spans-listint","text":"Encode a single annotation into IOB2 label indices. labels = encoder ( text = \"Did Dame Judy Dench star?\" , spans = [{ \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }], ) Parameters: Parameter Type Description text str The raw input text. spans list[dict] Span dicts with start/end/label fields (field names configurable via constructor). Returns: list[int] of IOB2 labels aligned to tokenizer output.","title":"__call__(text, spans) -&gt; list[int]"},{"location":"api/#batchannotations-on_errorraise-listlistint","text":"Encode a batch of annotations. Uses Rust-backed encode_batch() for parallelized tokenization. results = encoder . batch ( annotations , on_error = \"skip\" ) Parameters: Parameter Type Default Description annotations list[dict] required Annotation dicts with text/spans fields. on_error str \"raise\" \"raise\" to fail on first error, \"skip\" to skip failed annotations. Returns: list[list[int]] of IOB2 label sequences (unpadded).","title":"batch(annotations, *, on_error=\"raise\") -&gt; list[list[int]]"},{"location":"api/#decodelabels-encoding-text-listspan","text":"Recover span annotations from IOB2 label indices given a pre-built Encoding object. encoding = encoder . tokenizer . encode ( text ) spans = encoder . decode ( predicted_labels , encoding , text ) Parameters: Parameter Type Description labels list[int] IOB2 label indices (e.g., model predictions after argmax). encoding Encoding The tokenizers.Encoding object for the text. text str The original text (needed to resolve SentencePiece whitespace boundaries). Returns: list[Span] with character-level start , end , and label fields.","title":"decode(labels, encoding, text) -&gt; list[Span]"},{"location":"api/#decode_textlabels-text-listspan","text":"Convenience method: tokenizes the text internally, then decodes. spans = encoder . decode_text ( predicted_labels , text ) Parameters: Parameter Type Description labels list[int] IOB2 label indices. text str The raw input text (will be tokenized internally). Returns: list[Span] with character-level start , end , and label fields.","title":"decode_text(labels, text) -&gt; list[Span]"},{"location":"api/#types","text":"","title":"Types"},{"location":"api/#span","text":"A TypedDict representing a single entity annotation with character offsets. from iob2labels import Span Field Type Description start int Start character offset (inclusive). end int End character offset (exclusive). label str Entity class name.","title":"Span"},{"location":"api/#annotation","text":"A TypedDict representing a text with its entity annotations. from iob2labels import Annotation Field Type Description text str The annotated text. spans list[Span] Entity annotations.","title":"Annotation"},{"location":"api/#utility-functions","text":"","title":"Utility Functions"},{"location":"api/#create_label_maplabels-dictstr-int","text":"Build an IOB2 label-to-index mapping from a list of entity class names. from iob2labels import create_label_map label_map = create_label_map ([ \"actor\" , \"character\" ]) # {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4} Parameters: Parameter Type Default Description labels list[str] \\| None None Entity class names. Defaults to [\"LABEL\"] if None .","title":"create_label_map(labels) -&gt; dict[str, int]"},{"location":"api/#format_entity_labelprefix-label-str","text":"Format an IOB2 entity label string. from iob2labels import format_entity_label format_entity_label ( \"B\" , \"actor\" ) # 'B-ACTOR' Parameters: Parameter Type Description prefix \"B\" \\| \"I\" IOB2 prefix (Beginning or Inside). label str Entity class name.","title":"format_entity_label(prefix, label) -&gt; str"},{"location":"api/#preprocessingtext-spans-annotation","text":"Validate and normalize annotation data via Pydantic, then return as a typed dict. from iob2labels import preprocessing annotation = preprocessing ( text = \"Hello world\" , spans = [{ \"start\" : 0 , \"end\" : 5 , \"label\" : \"greeting\" }], ) Parameters: Parameter Type Default Description text str required The input text. spans list[dict] required Span dicts to validate. start_field str \"start\" Key for start offset. end_field str \"end\" Key for end offset. label_field str \"label\" Key for entity label. Raises: ValidationError for type mismatches, ValueError for invalid span geometry.","title":"preprocessing(text, spans, ...) -&gt; Annotation"},{"location":"api/#check_iob_conversion-none","text":"Verify that encoded IOB2 labels correctly recover the original entity text. Used internally when conversion_check=True . from iob2labels import check_iob_conversion check_iob_conversion ( iob_labels = labels , label_map = encoder . label_map , tokenizer = encoder . tokenizer , input_ids = encoding . ids , annotation = annotation , ) Parameters: Parameter Type Description iob_labels list[int] The encoded label sequence to verify. label_map dict[str, int] IOB2 label-to-index mapping. tokenizer Tokenizer The tokenizer instance. input_ids list[int] Token IDs from the encoding. annotation Annotation The original annotation for comparison. Raises: AssertionError if the recovered entities do not match the original spans.","title":"check_iob_conversion(...) -&gt; None"},{"location":"api/#get_entity_index_rangeslabel_map-iob_labels-listtupleint-int","text":"Extract token index ranges for each entity from an IOB2 label sequence. from iob2labels import get_entity_index_ranges ranges = get_entity_index_ranges ( encoder . label_map , labels ) # [(1, 4), (8, 8), (11, 12)] \u2014 (start_token_idx, end_token_idx) for each entity Parameters: Parameter Type Description label_map dict[str, int] IOB2 label-to-index mapping. iob_labels list[int] The IOB2 label sequence to scan.","title":"get_entity_index_ranges(label_map, iob_labels) -&gt; list[tuple[int, int]]"},{"location":"guide/","text":"User Guide Overview iob2labels converts character-offset NER span annotations (the format used by tools like Prodigy , Label Studio , and Doccano ) into integer label sequences aligned to any HuggingFace-compatible tokenizer. At inference time, it converts model predictions back into span annotations. The library depends only on tokenizers (HuggingFace Rust backend) and pydantic . No torch or transformers required. Installation uv add iob2labels Or with pip: pip install iob2labels Setting Up the Encoder The IOB2Encoder is the main interface. It requires two arguments: the entity class names and a tokenizer. from iob2labels import IOB2Encoder encoder = IOB2Encoder ( labels = [ \"actor\" , \"character\" , \"plot\" ], tokenizer = \"bert-base-uncased\" , ) The labels Parameter Pass a list of entity class names as strings. These are the NER categories in your annotation data. The encoder generates IOB2 tags from these labels: Each entity class produces 2 labels: B-{LABEL} (beginning) and I-{LABEL} (inside) Plus the O (outside) class Total label count is always (n * 2) + 1 encoder . label_map # {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4, 'B-PLOT': 5, 'I-PLOT': 6} The tokenizer Parameter The tokenizer argument accepts three forms: Checkpoint string \u2014 downloads the tokenizer from HuggingFace Hub. A UserWarning is emitted for checkpoints not in the tested list . encoder = IOB2Encoder ( labels = labels , tokenizer = \"bert-base-uncased\" ) tokenizers.Tokenizer instance \u2014 used directly. from tokenizers import Tokenizer tok = Tokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) transformers.PreTrainedTokenizerFast \u2014 the underlying tokenizers.Tokenizer is unwrapped automatically via the .backend_tokenizer attribute. from transformers import AutoTokenizer tok = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) Optional Parameters Parameter Type Default Description ignore_token int -100 Label value for special tokens ( [CLS] , [SEP] , etc.). PyTorch's CrossEntropyLoss ignores this value by default. conversion_check bool True Verify encoding correctness via round-trip check after every encoding. Disable for production performance. max_length int \\| None 512 Maximum token sequence length. Entities beyond the truncation boundary are skipped. Set to None to disable truncation. Encoding Annotations Single Annotation Call the encoder directly with text and spans : labels = encoder ( text = \"Did Dame Judy Dench star in a British film about Queen Elizabeth?\" , spans = [ { \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }, { \"label\" : \"plot\" , \"start\" : 30 , \"end\" : 37 }, { \"label\" : \"character\" , \"start\" : 49 , \"end\" : 64 }, ] ) # [-100, 0, 1, 2, 2, 2, 0, 0, 0, 5, 0, 0, 3, 4, 0, -100] Each integer in the output corresponds to a token from the tokenizer: -100 \u2014 special tokens ( [CLS] , [SEP] ), ignored during loss computation 0 \u2014 O (outside any entity) 1 \u2014 B-ACTOR (beginning of an actor entity) 2 \u2014 I-ACTOR (inside/continuation of an actor entity) And so on for each entity class Batch Encoding For multiple annotations, use batch() which leverages the Rust-backed encode_batch() for parallelized tokenization: annotations = [ { \"text\" : \"Did Dame Judy Dench star?\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }]}, { \"text\" : \"Matt Damon was Jason Bourne.\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 0 , \"end\" : 10 }]}, ] results = encoder . batch ( annotations ) # [[-100, 0, 1, 2, 2, 2, 0, -100], [-100, 1, 2, 0, 0, 0, 0, -100]] Results are returned without padding. Use HuggingFace's DataCollatorForTokenClassification or your own padding logic for training. The on_error parameter controls error handling: \"raise\" (default) \u2014 raise on the first error \"skip\" \u2014 skip failed annotations, return results for successful ones results = encoder . batch ( annotations , on_error = \"skip\" ) Decoding Predictions At inference time, convert model predictions (after argmax ) back into character-offset span annotations. From Raw Text Use decode_text() when you have the raw text but not the Encoding object: spans = encoder . decode_text ( predicted_labels , text ) # [{\"start\": 4, \"end\": 19, \"label\": \"actor\"}, ...] This tokenizes the text internally, then decodes. From a Pre-built Encoding Use decode() when you already have the tokenizers.Encoding object (avoids re-tokenizing): encoding = encoder . tokenizer . encode ( text ) spans = encoder . decode ( predicted_labels , encoding , text ) Both methods return list[Span] \u2014 a list of typed dicts with start , end , and label fields. SentencePiece whitespace handling Tokenizers like ALBERT, XLNet, T5, and XLM-RoBERTa absorb leading whitespace into tokens (e.g., \u2581Queen maps to chars (48, 54) instead of (49, 54) ). The decoder corrects these offsets automatically using the original text, so the returned spans always have accurate character boundaries. Working with Custom Data Formats Annotation tools use different field names. Configure the encoder to match your data format: # BioMed-NER dataset uses \"entities\" and \"class\" instead of \"spans\" and \"label\" encoder = IOB2Encoder ( labels = [ \"organism\" , \"chemicals\" ], tokenizer = \"bert-base-uncased\" , spans_field = \"entities\" , label_field = \"class\" , ) Available field name overrides: Parameter Default Description text_field \"text\" Key for the text string in batch annotation dicts spans_field \"spans\" Key for the spans list in batch annotation dicts start_field \"start\" Key for the start offset in span dicts end_field \"end\" Key for the end offset in span dicts label_field \"label\" Key for the entity label in span dicts Annotation Validation Input annotations are validated upfront with clear error messages: Negative offsets \u2014 start or end less than 0 Inverted spans \u2014 start >= end Out-of-bounds spans \u2014 end exceeds text length Overlapping spans \u2014 IOB2 does not support overlapping entities encoder ( text = \"Hello\" , spans = [{ \"label\" : \"test\" , \"start\" : 0 , \"end\" : 100 }]) # ValueError: Span 0 ('test') extends past the text (end=100, text length=5). # Ensure character offsets are within the text bounds. Conversion Checking By default, every encoding is verified by recovering the entity text from the produced labels and comparing it to the original annotation. This catches tokenizer misalignment bugs early. Disable it for production performance once you've verified correctness: encoder = IOB2Encoder ( labels = labels , tokenizer = tok , conversion_check = False , ) Converting to Tensors The encoder returns list[int] , which can be converted to any tensor format: import torch x = torch . tensor ( labels ) # or with numpy import numpy as np x = np . array ( labels ) For batched training, the sequences are unpadded. Use HuggingFace's DataCollatorForTokenClassification to handle padding and label alignment: from transformers import DataCollatorForTokenClassification collator = DataCollatorForTokenClassification ( tokenizer = hf_tokenizer )","title":"User Guide"},{"location":"guide/#user-guide","text":"","title":"User Guide"},{"location":"guide/#overview","text":"iob2labels converts character-offset NER span annotations (the format used by tools like Prodigy , Label Studio , and Doccano ) into integer label sequences aligned to any HuggingFace-compatible tokenizer. At inference time, it converts model predictions back into span annotations. The library depends only on tokenizers (HuggingFace Rust backend) and pydantic . No torch or transformers required.","title":"Overview"},{"location":"guide/#installation","text":"uv add iob2labels Or with pip: pip install iob2labels","title":"Installation"},{"location":"guide/#setting-up-the-encoder","text":"The IOB2Encoder is the main interface. It requires two arguments: the entity class names and a tokenizer. from iob2labels import IOB2Encoder encoder = IOB2Encoder ( labels = [ \"actor\" , \"character\" , \"plot\" ], tokenizer = \"bert-base-uncased\" , )","title":"Setting Up the Encoder"},{"location":"guide/#the-labels-parameter","text":"Pass a list of entity class names as strings. These are the NER categories in your annotation data. The encoder generates IOB2 tags from these labels: Each entity class produces 2 labels: B-{LABEL} (beginning) and I-{LABEL} (inside) Plus the O (outside) class Total label count is always (n * 2) + 1 encoder . label_map # {'O': 0, 'B-ACTOR': 1, 'I-ACTOR': 2, 'B-CHARACTER': 3, 'I-CHARACTER': 4, 'B-PLOT': 5, 'I-PLOT': 6}","title":"The labels Parameter"},{"location":"guide/#the-tokenizer-parameter","text":"The tokenizer argument accepts three forms: Checkpoint string \u2014 downloads the tokenizer from HuggingFace Hub. A UserWarning is emitted for checkpoints not in the tested list . encoder = IOB2Encoder ( labels = labels , tokenizer = \"bert-base-uncased\" ) tokenizers.Tokenizer instance \u2014 used directly. from tokenizers import Tokenizer tok = Tokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok ) transformers.PreTrainedTokenizerFast \u2014 the underlying tokenizers.Tokenizer is unwrapped automatically via the .backend_tokenizer attribute. from transformers import AutoTokenizer tok = AutoTokenizer . from_pretrained ( \"bert-base-uncased\" ) encoder = IOB2Encoder ( labels = labels , tokenizer = tok )","title":"The tokenizer Parameter"},{"location":"guide/#optional-parameters","text":"Parameter Type Default Description ignore_token int -100 Label value for special tokens ( [CLS] , [SEP] , etc.). PyTorch's CrossEntropyLoss ignores this value by default. conversion_check bool True Verify encoding correctness via round-trip check after every encoding. Disable for production performance. max_length int \\| None 512 Maximum token sequence length. Entities beyond the truncation boundary are skipped. Set to None to disable truncation.","title":"Optional Parameters"},{"location":"guide/#encoding-annotations","text":"","title":"Encoding Annotations"},{"location":"guide/#single-annotation","text":"Call the encoder directly with text and spans : labels = encoder ( text = \"Did Dame Judy Dench star in a British film about Queen Elizabeth?\" , spans = [ { \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }, { \"label\" : \"plot\" , \"start\" : 30 , \"end\" : 37 }, { \"label\" : \"character\" , \"start\" : 49 , \"end\" : 64 }, ] ) # [-100, 0, 1, 2, 2, 2, 0, 0, 0, 5, 0, 0, 3, 4, 0, -100] Each integer in the output corresponds to a token from the tokenizer: -100 \u2014 special tokens ( [CLS] , [SEP] ), ignored during loss computation 0 \u2014 O (outside any entity) 1 \u2014 B-ACTOR (beginning of an actor entity) 2 \u2014 I-ACTOR (inside/continuation of an actor entity) And so on for each entity class","title":"Single Annotation"},{"location":"guide/#batch-encoding","text":"For multiple annotations, use batch() which leverages the Rust-backed encode_batch() for parallelized tokenization: annotations = [ { \"text\" : \"Did Dame Judy Dench star?\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 4 , \"end\" : 19 }]}, { \"text\" : \"Matt Damon was Jason Bourne.\" , \"spans\" : [{ \"label\" : \"actor\" , \"start\" : 0 , \"end\" : 10 }]}, ] results = encoder . batch ( annotations ) # [[-100, 0, 1, 2, 2, 2, 0, -100], [-100, 1, 2, 0, 0, 0, 0, -100]] Results are returned without padding. Use HuggingFace's DataCollatorForTokenClassification or your own padding logic for training. The on_error parameter controls error handling: \"raise\" (default) \u2014 raise on the first error \"skip\" \u2014 skip failed annotations, return results for successful ones results = encoder . batch ( annotations , on_error = \"skip\" )","title":"Batch Encoding"},{"location":"guide/#decoding-predictions","text":"At inference time, convert model predictions (after argmax ) back into character-offset span annotations.","title":"Decoding Predictions"},{"location":"guide/#from-raw-text","text":"Use decode_text() when you have the raw text but not the Encoding object: spans = encoder . decode_text ( predicted_labels , text ) # [{\"start\": 4, \"end\": 19, \"label\": \"actor\"}, ...] This tokenizes the text internally, then decodes.","title":"From Raw Text"},{"location":"guide/#from-a-pre-built-encoding","text":"Use decode() when you already have the tokenizers.Encoding object (avoids re-tokenizing): encoding = encoder . tokenizer . encode ( text ) spans = encoder . decode ( predicted_labels , encoding , text ) Both methods return list[Span] \u2014 a list of typed dicts with start , end , and label fields. SentencePiece whitespace handling Tokenizers like ALBERT, XLNet, T5, and XLM-RoBERTa absorb leading whitespace into tokens (e.g., \u2581Queen maps to chars (48, 54) instead of (49, 54) ). The decoder corrects these offsets automatically using the original text, so the returned spans always have accurate character boundaries.","title":"From a Pre-built Encoding"},{"location":"guide/#working-with-custom-data-formats","text":"Annotation tools use different field names. Configure the encoder to match your data format: # BioMed-NER dataset uses \"entities\" and \"class\" instead of \"spans\" and \"label\" encoder = IOB2Encoder ( labels = [ \"organism\" , \"chemicals\" ], tokenizer = \"bert-base-uncased\" , spans_field = \"entities\" , label_field = \"class\" , ) Available field name overrides: Parameter Default Description text_field \"text\" Key for the text string in batch annotation dicts spans_field \"spans\" Key for the spans list in batch annotation dicts start_field \"start\" Key for the start offset in span dicts end_field \"end\" Key for the end offset in span dicts label_field \"label\" Key for the entity label in span dicts","title":"Working with Custom Data Formats"},{"location":"guide/#annotation-validation","text":"Input annotations are validated upfront with clear error messages: Negative offsets \u2014 start or end less than 0 Inverted spans \u2014 start >= end Out-of-bounds spans \u2014 end exceeds text length Overlapping spans \u2014 IOB2 does not support overlapping entities encoder ( text = \"Hello\" , spans = [{ \"label\" : \"test\" , \"start\" : 0 , \"end\" : 100 }]) # ValueError: Span 0 ('test') extends past the text (end=100, text length=5). # Ensure character offsets are within the text bounds.","title":"Annotation Validation"},{"location":"guide/#conversion-checking","text":"By default, every encoding is verified by recovering the entity text from the produced labels and comparing it to the original annotation. This catches tokenizer misalignment bugs early. Disable it for production performance once you've verified correctness: encoder = IOB2Encoder ( labels = labels , tokenizer = tok , conversion_check = False , )","title":"Conversion Checking"},{"location":"guide/#converting-to-tensors","text":"The encoder returns list[int] , which can be converted to any tensor format: import torch x = torch . tensor ( labels ) # or with numpy import numpy as np x = np . array ( labels ) For batched training, the sequences are unpadded. Use HuggingFace's DataCollatorForTokenClassification to handle padding and label alignment: from transformers import DataCollatorForTokenClassification collator = DataCollatorForTokenClassification ( tokenizer = hf_tokenizer )","title":"Converting to Tensors"},{"location":"tokenizers/","text":"Supported Tokenizers iob2labels is tested across four tokenizer families covering 18 checkpoints. Both encoding and decoding are verified via round-trip tests: encode spans to labels, decode labels back to spans, and assert the recovered spans exactly match the originals. Tested Checkpoints Family Checkpoints WordPiece bert-base-cased , bert-base-uncased , bert-large-cased , bert-large-uncased , bert-base-multilingual-cased , distilbert-base-cased , distilbert-base-uncased , google/electra-base-discriminator BPE (byte-level) roberta-base , roberta-large , distilroberta-base , allenai/longformer-base-4096 SentencePiece BPE FacebookAI/xlm-roberta-base , almanach/camembert-base SentencePiece Unigram albert-base-v2 , xlnet-base-cased , t5-small , google/flan-t5-base Tokenizer Families WordPiece WordPiece tokenizers (BERT, DistilBERT, ELECTRA) split words into subword units prefixed with ## . For example, \"tokenization\" might become [\"token\", \"##ization\"] . Character-to-token alignment is straightforward \u2014 each subword maps cleanly to a contiguous character range. These tokenizers are fully compatible with iob2labels and require no special handling. BPE (Byte-Level) Byte-level BPE tokenizers (RoBERTa, Longformer) operate on byte-level representations and use \\u0120 (\u0120) as a word-boundary prefix. For example, \" Queen\" becomes [\"\u0120Queen\"] . The Encoding.char_to_token() and Encoding.token_to_chars() methods handle the byte-level mapping correctly, so iob2labels works with these tokenizers without modification. SentencePiece BPE SentencePiece BPE tokenizers (XLM-RoBERTa, CamemBERT) use the \u2581 (U+2581) character to represent word boundaries. For example, \" Queen\" becomes [\"\u2581Queen\"] . These tokenizers absorb the leading whitespace into the token, which means token_to_chars() returns a range that includes the space character. The decoder handles this automatically \u2014 see Whitespace Handling below. SentencePiece Unigram SentencePiece Unigram tokenizers (ALBERT, XLNet, T5, Flan-T5) also use the \u2581 prefix convention and share the same whitespace absorption behavior as SentencePiece BPE. Google Flan-T5 sentinel tokens Flan-T5 uses sentinel tokens (e.g., <extra_id_0> ) in its vocabulary. The round-trip check accounts for these during conversion verification. SentencePiece Whitespace Handling SentencePiece-based tokenizers (both BPE and Unigram families) absorb leading whitespace into the first token of each word. This means token_to_chars() can return character offsets that include the space before the entity rather than just the entity text. For example, given the text \"film about Queen Elizabeth\" and an entity at (11, 26) covering \"Queen Elizabeth\" : A WordPiece tokenizer maps \"Queen\" to chars (11, 16) \u2014 correct A SentencePiece tokenizer maps \"\u2581Queen\" to chars (10, 16) \u2014 includes the preceding space The decoder corrects this automatically by stripping leading (and trailing) whitespace from the recovered character range using the original text. No configuration needed. Using Untested Tokenizers Any HuggingFace-compatible tokenizer with a tokenizer.json file on the Hub should work with iob2labels . When you use a checkpoint not in the tested list, a UserWarning is emitted: UserWarning: Tokenizer 'my-custom/tokenizer' is not in the list of checkpoints tested with iob2labels. It may work correctly, but results have not been verified. The built-in conversion check ( conversion_check=True , the default) will catch any alignment issues at encoding time. If the round-trip verification passes, the tokenizer is working correctly for your data. To suppress the warning once you've verified correctness: import warnings warnings . filterwarnings ( \"ignore\" , message = \"Tokenizer.*not in the list\" )","title":"Supported Tokenizers"},{"location":"tokenizers/#supported-tokenizers","text":"iob2labels is tested across four tokenizer families covering 18 checkpoints. Both encoding and decoding are verified via round-trip tests: encode spans to labels, decode labels back to spans, and assert the recovered spans exactly match the originals.","title":"Supported Tokenizers"},{"location":"tokenizers/#tested-checkpoints","text":"Family Checkpoints WordPiece bert-base-cased , bert-base-uncased , bert-large-cased , bert-large-uncased , bert-base-multilingual-cased , distilbert-base-cased , distilbert-base-uncased , google/electra-base-discriminator BPE (byte-level) roberta-base , roberta-large , distilroberta-base , allenai/longformer-base-4096 SentencePiece BPE FacebookAI/xlm-roberta-base , almanach/camembert-base SentencePiece Unigram albert-base-v2 , xlnet-base-cased , t5-small , google/flan-t5-base","title":"Tested Checkpoints"},{"location":"tokenizers/#tokenizer-families","text":"","title":"Tokenizer Families"},{"location":"tokenizers/#wordpiece","text":"WordPiece tokenizers (BERT, DistilBERT, ELECTRA) split words into subword units prefixed with ## . For example, \"tokenization\" might become [\"token\", \"##ization\"] . Character-to-token alignment is straightforward \u2014 each subword maps cleanly to a contiguous character range. These tokenizers are fully compatible with iob2labels and require no special handling.","title":"WordPiece"},{"location":"tokenizers/#bpe-byte-level","text":"Byte-level BPE tokenizers (RoBERTa, Longformer) operate on byte-level representations and use \\u0120 (\u0120) as a word-boundary prefix. For example, \" Queen\" becomes [\"\u0120Queen\"] . The Encoding.char_to_token() and Encoding.token_to_chars() methods handle the byte-level mapping correctly, so iob2labels works with these tokenizers without modification.","title":"BPE (Byte-Level)"},{"location":"tokenizers/#sentencepiece-bpe","text":"SentencePiece BPE tokenizers (XLM-RoBERTa, CamemBERT) use the \u2581 (U+2581) character to represent word boundaries. For example, \" Queen\" becomes [\"\u2581Queen\"] . These tokenizers absorb the leading whitespace into the token, which means token_to_chars() returns a range that includes the space character. The decoder handles this automatically \u2014 see Whitespace Handling below.","title":"SentencePiece BPE"},{"location":"tokenizers/#sentencepiece-unigram","text":"SentencePiece Unigram tokenizers (ALBERT, XLNet, T5, Flan-T5) also use the \u2581 prefix convention and share the same whitespace absorption behavior as SentencePiece BPE. Google Flan-T5 sentinel tokens Flan-T5 uses sentinel tokens (e.g., <extra_id_0> ) in its vocabulary. The round-trip check accounts for these during conversion verification.","title":"SentencePiece Unigram"},{"location":"tokenizers/#sentencepiece-whitespace-handling","text":"SentencePiece-based tokenizers (both BPE and Unigram families) absorb leading whitespace into the first token of each word. This means token_to_chars() can return character offsets that include the space before the entity rather than just the entity text. For example, given the text \"film about Queen Elizabeth\" and an entity at (11, 26) covering \"Queen Elizabeth\" : A WordPiece tokenizer maps \"Queen\" to chars (11, 16) \u2014 correct A SentencePiece tokenizer maps \"\u2581Queen\" to chars (10, 16) \u2014 includes the preceding space The decoder corrects this automatically by stripping leading (and trailing) whitespace from the recovered character range using the original text. No configuration needed.","title":"SentencePiece Whitespace Handling"},{"location":"tokenizers/#using-untested-tokenizers","text":"Any HuggingFace-compatible tokenizer with a tokenizer.json file on the Hub should work with iob2labels . When you use a checkpoint not in the tested list, a UserWarning is emitted: UserWarning: Tokenizer 'my-custom/tokenizer' is not in the list of checkpoints tested with iob2labels. It may work correctly, but results have not been verified. The built-in conversion check ( conversion_check=True , the default) will catch any alignment issues at encoding time. If the round-trip verification passes, the tokenizer is working correctly for your data. To suppress the warning once you've verified correctness: import warnings warnings . filterwarnings ( \"ignore\" , message = \"Tokenizer.*not in the list\" )","title":"Using Untested Tokenizers"}]}